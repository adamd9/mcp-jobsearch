# MCP Job Search

This project implements a LinkedIn job scraper with persistent job indexing, deep scanning, and filtering capabilities. It scrapes LinkedIn job listings, performs detailed analysis of each job against a candidate profile using OpenAI, stores matches in a persistent job index, and exposes MCP-compatible HTTP endpoints.

## Implementation Options

This project provides two separate implementations:

1. **Node.js Implementation** (Original): Located in the `src/` directory
   - Full-featured with filesystem access, screenshots, and raw file storage
   - Runs locally or on any Node.js hosting platform

2. **Cloudflare Worker Implementation** (New): Located in the `workers/` directory
   - Serverless implementation using Cloudflare Workers
   - Uses Cloudflare's Playwright fork for scraping
   - Stores data in Cloudflare KV
   - No filesystem access (no screenshots or raw file storage)

## Node.js Implementation

### Setup

1. Copy `.env.example` to `.env` and fill in your credentials:
   ```
   LINKEDIN_EMAIL=your-linkedin-email@example.com
   LINKEDIN_PASSWORD=your-linkedin-password
   OPENAI_API_KEY=your-openai-api-key
   OPENAI_MODEL=gpt-4o
   DEEP_SCAN_CONCURRENCY=2
   SMTP_HOST=smtp.example.com
   SMTP_PORT=587
   SMTP_USER=your-smtp-username
   SMTP_PASS=your-smtp-password
   DIGEST_FROM=jobs@example.com
   DIGEST_TO=you@example.com
   TIMEZONE=Australia/Sydney
   ACCESS_TOKEN=your-secure-random-token
   ```
   
   The `ACCESS_TOKEN` is used for API authentication and should be a secure random string.

2. Run `./setup.sh` to install npm packages and Playwright's browser dependencies.
3. Create a `plan.json` file (or use the `/plan` endpoint) describing your profile, search terms and deep scan criteria.
4. Start the server with `npm start`.

### Core Features
- **Plan Driven Search**: Define your profile, search terms and scan prompt in `plan.json` or via the `/plan` API.
- **Persistent Job Index**: All scraped jobs are stored in a persistent JSON file (`data/job-index.json`).
- **Deep Scanning**: Visits each job posting to extract comprehensive details and analyze with OpenAI.
- **Email Digests**: Sends email digests of matched jobs.

For more details on the Node.js implementation, see the [original documentation](#persistent-job-index).

## Cloudflare Worker Implementation

### Setup

1. Navigate to the `workers/` directory:
   ```bash
   cd workers
   ```

2. Install dependencies:
   ```bash
   npm install
   ```

3. Login to Cloudflare:
   ```bash
   npx wrangler login
   ```

4. Create a KV namespace for job storage:
   ```bash
   npx wrangler kv:namespace create JOB_STORAGE
   ```

5. Update `wrangler.toml` with your KV namespace ID and environment variables.

### Local Development

To run the Worker locally:

```bash
cd workers
npm run dev
```

This will start a local development server at http://localhost:8787.

### Deployment

To deploy to Cloudflare Workers:

```bash
cd workers
npm run deploy
```

### Deploy to Cloudflare Button

[![Deploy to Cloudflare Workers](https://deploy.workers.cloudflare.com/button)](https://deploy.workers.cloudflare.com/?url=https://github.com/YOUR_USERNAME/mcp-jobsearch)

To use the Deploy to Cloudflare button:
1. Fork this repository
2. Update the URL in the button above with your GitHub username
3. Click the button to deploy to your Cloudflare account
4. Configure the required environment variables during deployment

## Feature Comparison

| Feature | Node.js Implementation | Cloudflare Worker |
|---------|------------------------|-------------------|
| LinkedIn Scraping | ✅ | ✅ |
| Job Analysis with OpenAI | ✅ | ✅ |
| Persistent Storage | ✅ (Filesystem) | ✅ (KV) |
| Deep Scanning | ✅ | ✅ |
| Screenshots | ✅ | ❌ |
| Raw File Storage | ✅ | ❌ |
| Email Digests | ✅ | ❌ |
| MCP Server | ✅ | ✅ |
| Deploy to Cloudflare Button | ❌ | ✅ |

## Authentication

All API endpoints in both implementations require authentication using the `ACCESS_TOKEN`. Include the token in the `Authorization` header with each request:

```
Authorization: Bearer your-access-token-here
```

Requests without a valid authentication token will receive a 401 Unauthorized response.

---

The rest of this README contains detailed information about the original Node.js implementation.

## Persistent Job Index
- **Storage**: All scraped jobs are stored in a persistent JSON file (`data/job-index.json`).
- **Deduplication**: Jobs are uniquely identified by LinkedIn job ID to prevent duplicate scanning.
- **Profile Change Detection**: System detects when your profile changes and triggers rescans.
- **Metadata**: Each job entry includes scan status, match score, and detailed information.

#### Job Index Structure
Each job in `data/job-index.json` keeps the basic listing data along with the
results of the most recent deep scan:

```json
{
  "id": "123456",
  "title": "Full Stack Engineer",
  "company": "ExampleCo",
  "link": "https://linkedin.com/jobs/view/123456",
  "posted": "2025-06-09",
  "scanned": true,
  "scanDate": "2025-07-05T12:00:00+10:00",
  "matchScore": 0.85,
  "matchReason": "Good skills overlap with your profile",
  "description": "Full job description...",
  "requirements": ["Skill 1", "Skill 2"],
  "location": "Sydney, Australia",
  "salary": "$100k - $120k"
}
```

After each deep scan the `matchScore` and `matchReason` are updated so you can
see why a job was scored the way it was. When a job is rescanned (for example
after updating your profile) you may choose to store multiple scores in an array
so previous results are preserved:

```json
{
  "scanHistory": [
    { "date": "2025-07-05T12:00:00+10:00", "score": 0.85,
      "summary": "Good skills overlap with your profile" },
    { "date": "2025-07-10T12:00:00+10:00", "score": 0.88,
      "summary": "Profile updated with React experience" }
  ]
}
```
